#!/usr/bin/env python
"""
ä½¿ç”¨ç¤ºä¾‹ï¼šè®­ç»ƒ My Custom Policy

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ my_custom_policy è¿›è¡Œè®­ç»ƒã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ï¼‰
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import torch
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.policies.tactile_diffusion import MyCustomPolicy, MyCustomPolicyConfig


def train_example():
    """
    è®­ç»ƒç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    å®é™…è®­ç»ƒè¯·ä½¿ç”¨:
    lerobot-train --policy.type my_custom_policy ...
    """
    
    print("=" * 80)
    print("My Custom Policy è®­ç»ƒç¤ºä¾‹")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®é›†
    print("\nã€1ã€‘åŠ è½½æ•°æ®é›†")
    dataset = LeRobotDataset(
        repo_id="Opendrawer",
        root="./datasets/test_data/xarm_leap_tactile_lift_blind"
    )
    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"  æ€» Episodes: {dataset.num_episodes}")
    print(f"  æ€» Frames: {dataset.num_frames}")
    
    # 2. åˆ›å»ºé…ç½®
    print("\nã€2ã€‘åˆ›å»ºPolicyé…ç½®")
    from lerobot.datasets.utils import dataset_to_policy_features
    
    # ä»æ•°æ®é›†ç‰¹å¾åˆ›å»ºpolicyç‰¹å¾
    policy_features = dataset_to_policy_features(dataset.features)
    
    # åˆ†ç¦»è¾“å…¥å’Œè¾“å‡ºç‰¹å¾
    input_features = {k: v for k, v in policy_features.items() if k.startswith("observation")}
    output_features = {k: v for k, v in policy_features.items() if k.startswith("action")}
    
    config = MyCustomPolicyConfig(
        input_features=input_features,
        output_features=output_features,
        use_tactile_features=True,
        tactile_encoder_hidden_dim=64,
        n_obs_steps=2,
        horizon=16,
        n_action_steps=8,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    
    print(f"âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"  è¾“å…¥ç‰¹å¾: {list(input_features.keys())}")
    print(f"  è¾“å‡ºç‰¹å¾: {list(output_features.keys())}")
    print(f"  Device: {config.device}")
    
    # 3. å®ä¾‹åŒ–Policy
    print("\nã€3ã€‘å®ä¾‹åŒ–Policy")
    policy = MyCustomPolicy(config)
    print(f"âœ“ Policyåˆ›å»ºæˆåŠŸ")
    print(f"  å‚æ•°æ€»æ•°: {sum(p.numel() for p in policy.parameters()):,}")
    
    # 4. å‡†å¤‡è®­ç»ƒ
    print("\nã€4ã€‘è®­ç»ƒå‡†å¤‡")
    optimizer = torch.optim.Adam(policy.get_optim_params(), lr=1e-4)
    
    # è·å–ä¸€ä¸ªbatchç¤ºä¾‹
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        num_workers=0,
    )
    
    batch = next(iter(dataloader))
    
    # å°†æ•°æ®ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
    batch = {k: v.to(config.device) if isinstance(v, torch.Tensor) else v 
             for k, v in batch.items()}
    
    print(f"âœ“ è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"  Batch keys: {list(batch.keys())}")
    print(f"  observation.state shape: {batch['observation.state'].shape}")
    if 'observation.tactile' in batch:
        print(f"  observation.tactile shape: {batch['observation.tactile'].shape}")
    print(f"  action shape: {batch['action'].shape}")
    
    # 5. æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤
    print("\nã€5ã€‘æ‰§è¡Œè®­ç»ƒæ­¥éª¤")
    policy.train()
    
    loss, _ = policy.forward(batch)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    print(f"âœ“ è®­ç»ƒæ­¥éª¤å®Œæˆ")
    print(f"  Loss: {loss.item():.6f}")
    
    # 6. æ‰§è¡Œæ¨ç†
    print("\nã€6ã€‘æ‰§è¡Œæ¨ç†æµ‹è¯•")
    policy.eval()
    policy.reset()
    
    with torch.no_grad():
        # å‡†å¤‡å•ä¸ªè§‚å¯Ÿ
        obs = {
            k: v[0:1] for k, v in batch.items() 
            if k.startswith("observation")
        }
        
        # é€‰æ‹©åŠ¨ä½œ
        action = policy.select_action(obs)
        
    print(f"âœ“ æ¨ç†æµ‹è¯•å®Œæˆ")
    print(f"  Action shape: {action.shape}")
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼")
    print("\nğŸ’¡ å®é™…è®­ç»ƒè¯·ä½¿ç”¨:")
    print("   lerobot-train \\")
    print("       --policy.type my_custom_policy \\")
    print("       --policy.use_tactile_features=true \\")
    print("       --dataset.repo_id Opendrawer \\")
    print("       --dataset.root ./datasets/test_data/xarm_leap_tactile_lift_blind \\")
    print("       --steps 200000")
    print("=" * 80)


if __name__ == "__main__":
    try:
        train_example()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)